{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "import os\n",
    "from glob import glob\n",
    "import cv2\n",
    "import numpy as np\n",
    "from scipy.fft import fft\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load labeled frames and their corresponding labels\n",
    "def load_labeled_frames(data_folder):\n",
    "    images = []\n",
    "    labels = []\n",
    "\n",
    "    for image_path in glob(os.path.join(data_folder, '*.jpg')):\n",
    "        image = cv2.imread(image_path)\n",
    "        image = cv2.resize(image, (30, 30))\n",
    "\n",
    "        label = os.path.splitext(os.path.basename(image_path))[0].split('_')[-1]\n",
    "\n",
    "        images.append(image)\n",
    "\n",
    "        if label == '0':\n",
    "            labels.append(0)\n",
    "        elif label == '1':\n",
    "            labels.append(1)\n",
    "\n",
    "    return images, labels\n",
    "\n",
    "# Preprocess the data\n",
    "def preprocess_data(images, labels):\n",
    "    images = np.array(images)\n",
    "    labels = np.array(labels)\n",
    "    \n",
    "    # duplicate the images and labels\n",
    "    images = np.concatenate((images, images), axis=0)\n",
    "    labels = np.concatenate((labels, labels), axis=0)\n",
    "\n",
    "    # Normalize pixel values to be between 0 and 1\n",
    "    images = images / 255.0\n",
    "\n",
    "    # Shuffle and split the data into training and testing sets\n",
    "    images, labels = shuffle(images, labels, random_state=46)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(images, labels, test_size=0.2, random_state=45)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30, 30, 3)\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "output_folder = 'labeled_frames_binary_test'\n",
    "\n",
    "# Load labeled frames and their corresponding labels\n",
    "images, labels = load_labeled_frames(output_folder)\n",
    "\n",
    "# Preprocess the data\n",
    "X_train, X_test, y_train, y_test = preprocess_data(images, labels)\n",
    "\n",
    "input_shape = X_train[0].shape\n",
    "print(input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1,\n",
       "       1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1,\n",
       "       1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1,\n",
       "       1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "3/3 [==============================] - 3s 323ms/step - loss: 0.6640 - accuracy: 0.5814 - auc: 0.6015 - val_loss: 0.5670 - val_accuracy: 0.6364 - val_auc: 1.0000\n",
      "Epoch 2/30\n",
      "3/3 [==============================] - 0s 62ms/step - loss: 0.5365 - accuracy: 0.6512 - auc: 0.8574 - val_loss: 0.4509 - val_accuracy: 0.6364 - val_auc: 1.0000\n",
      "Epoch 3/30\n",
      "3/3 [==============================] - 0s 58ms/step - loss: 0.5007 - accuracy: 0.6512 - auc: 0.8664 - val_loss: 0.4183 - val_accuracy: 0.6364 - val_auc: 1.0000\n",
      "Epoch 4/30\n",
      "3/3 [==============================] - 0s 57ms/step - loss: 0.4376 - accuracy: 0.6512 - auc: 0.9494 - val_loss: 0.4107 - val_accuracy: 0.6364 - val_auc: 1.0000\n",
      "Epoch 5/30\n",
      "3/3 [==============================] - 0s 49ms/step - loss: 0.3880 - accuracy: 0.6512 - auc: 0.9812 - val_loss: 0.3736 - val_accuracy: 0.6364 - val_auc: 1.0000\n",
      "Epoch 6/30\n",
      "3/3 [==============================] - 0s 50ms/step - loss: 0.3453 - accuracy: 0.6628 - auc: 0.9881 - val_loss: 0.3375 - val_accuracy: 1.0000 - val_auc: 1.0000\n",
      "Epoch 7/30\n",
      "3/3 [==============================] - 0s 48ms/step - loss: 0.3035 - accuracy: 0.7907 - auc: 0.9988 - val_loss: 0.3167 - val_accuracy: 1.0000 - val_auc: 1.0000\n",
      "Epoch 8/30\n",
      "3/3 [==============================] - 0s 52ms/step - loss: 0.2816 - accuracy: 0.7907 - auc: 0.9994 - val_loss: 0.2948 - val_accuracy: 1.0000 - val_auc: 1.0000\n",
      "Epoch 9/30\n",
      "3/3 [==============================] - 0s 71ms/step - loss: 0.2602 - accuracy: 0.8953 - auc: 1.0000 - val_loss: 0.2850 - val_accuracy: 1.0000 - val_auc: 1.0000\n",
      "Epoch 10/30\n",
      "3/3 [==============================] - 0s 62ms/step - loss: 0.2492 - accuracy: 0.9651 - auc: 1.0000 - val_loss: 0.2730 - val_accuracy: 1.0000 - val_auc: 1.0000\n",
      "Epoch 11/30\n",
      "3/3 [==============================] - 0s 60ms/step - loss: 0.2414 - accuracy: 0.9767 - auc: 1.0000 - val_loss: 0.2642 - val_accuracy: 1.0000 - val_auc: 1.0000\n",
      "Epoch 12/30\n",
      "3/3 [==============================] - 0s 59ms/step - loss: 0.2368 - accuracy: 1.0000 - auc: 1.0000 - val_loss: 0.2559 - val_accuracy: 1.0000 - val_auc: 1.0000\n",
      "Epoch 13/30\n",
      "3/3 [==============================] - 0s 57ms/step - loss: 0.2320 - accuracy: 1.0000 - auc: 1.0000 - val_loss: 0.2488 - val_accuracy: 1.0000 - val_auc: 1.0000\n",
      "Epoch 14/30\n",
      "3/3 [==============================] - 0s 62ms/step - loss: 0.2272 - accuracy: 1.0000 - auc: 1.0000 - val_loss: 0.2428 - val_accuracy: 1.0000 - val_auc: 1.0000\n",
      "Epoch 15/30\n",
      "3/3 [==============================] - 0s 58ms/step - loss: 0.2212 - accuracy: 1.0000 - auc: 1.0000 - val_loss: 0.2363 - val_accuracy: 1.0000 - val_auc: 1.0000\n",
      "Epoch 16/30\n",
      "3/3 [==============================] - 0s 63ms/step - loss: 0.2134 - accuracy: 1.0000 - auc: 1.0000 - val_loss: 0.2290 - val_accuracy: 1.0000 - val_auc: 1.0000\n",
      "Epoch 17/30\n",
      "3/3 [==============================] - 0s 66ms/step - loss: 0.2008 - accuracy: 1.0000 - auc: 1.0000 - val_loss: 0.2211 - val_accuracy: 1.0000 - val_auc: 1.0000\n",
      "Epoch 18/30\n",
      "3/3 [==============================] - 0s 64ms/step - loss: 0.1827 - accuracy: 1.0000 - auc: 1.0000 - val_loss: 0.2085 - val_accuracy: 1.0000 - val_auc: 1.0000\n",
      "Epoch 19/30\n",
      "3/3 [==============================] - 0s 74ms/step - loss: 0.1634 - accuracy: 1.0000 - auc: 1.0000 - val_loss: 0.1893 - val_accuracy: 1.0000 - val_auc: 1.0000\n",
      "Epoch 20/30\n",
      "3/3 [==============================] - 0s 64ms/step - loss: 0.1800 - accuracy: 0.9651 - auc: 1.0000 - val_loss: 0.1783 - val_accuracy: 1.0000 - val_auc: 1.0000\n",
      "Epoch 21/30\n",
      "3/3 [==============================] - 0s 71ms/step - loss: 0.2120 - accuracy: 0.9767 - auc: 0.9643 - val_loss: 0.1664 - val_accuracy: 1.0000 - val_auc: 1.0000\n",
      "Epoch 22/30\n",
      "3/3 [==============================] - 0s 84ms/step - loss: 0.1144 - accuracy: 0.9884 - auc: 1.0000 - val_loss: 0.1475 - val_accuracy: 1.0000 - val_auc: 1.0000\n",
      "Epoch 23/30\n",
      "3/3 [==============================] - 0s 64ms/step - loss: 0.1292 - accuracy: 0.9651 - auc: 1.0000 - val_loss: 0.1235 - val_accuracy: 1.0000 - val_auc: 1.0000\n",
      "Epoch 24/30\n",
      "3/3 [==============================] - 0s 76ms/step - loss: 0.0736 - accuracy: 1.0000 - auc: 1.0000 - val_loss: 0.0938 - val_accuracy: 1.0000 - val_auc: 1.0000\n",
      "Epoch 25/30\n",
      "3/3 [==============================] - 0s 69ms/step - loss: 0.0504 - accuracy: 1.0000 - auc: 1.0000 - val_loss: 0.0634 - val_accuracy: 1.0000 - val_auc: 1.0000\n",
      "Epoch 26/30\n",
      "3/3 [==============================] - 0s 65ms/step - loss: 0.0309 - accuracy: 1.0000 - auc: 1.0000 - val_loss: 0.0380 - val_accuracy: 1.0000 - val_auc: 1.0000\n",
      "Epoch 27/30\n",
      "3/3 [==============================] - 0s 65ms/step - loss: 0.0147 - accuracy: 1.0000 - auc: 1.0000 - val_loss: 0.0226 - val_accuracy: 1.0000 - val_auc: 1.0000\n",
      "Epoch 28/30\n",
      "3/3 [==============================] - 0s 82ms/step - loss: 0.0096 - accuracy: 1.0000 - auc: 1.0000 - val_loss: 0.0149 - val_accuracy: 1.0000 - val_auc: 1.0000\n",
      "Epoch 29/30\n",
      "3/3 [==============================] - 0s 75ms/step - loss: 0.0061 - accuracy: 1.0000 - auc: 1.0000 - val_loss: 0.0105 - val_accuracy: 1.0000 - val_auc: 1.0000\n",
      "Epoch 30/30\n",
      "3/3 [==============================] - 0s 64ms/step - loss: 0.0038 - accuracy: 1.0000 - auc: 1.0000 - val_loss: 0.0057 - val_accuracy: 1.0000 - val_auc: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# Build the model\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape))\n",
    "model.add(layers.MaxPooling2D())\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D())\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D())\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(128, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(),\n",
    "              loss='binary_crossentropy',  # Use binary crossentropy for binary classification\n",
    "              metrics=['accuracy', 'AUC'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=30, validation_data=(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 - 3s - loss: 0.0059 - accuracy: 1.0000 - auc: 1.0000 - 3s/epoch - 3s/step\n",
      "\n",
      "Test AUC: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "test_loss, test_acc, test_auc = model.evaluate(X_test, y_test, verbose=2)\n",
    "\n",
    "print('\\nTest AUC:', test_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "framelist = []\n",
    "data_folder = 'labeled_frames_fixed'\n",
    "video = './vidscapstone/vid12_calibration.mp4'\n",
    "\n",
    "for image_path in glob(os.path.join(data_folder, '*.jpg')):\n",
    "    framelist.append(image_path)\n",
    "\n",
    "# framelist = [\n",
    "#              'labeled_frames_fixed\\\\vid9.mp4_00054_2.jpg', \n",
    "#              'labeled_frames_fixed\\\\vid9.mp4_00047_0.jpg', \n",
    "#              'labeled_frames_fixed\\\\vid9.mp4_00053_3.jpg', \n",
    "#              'labeled_frames_fixed\\\\vid9.mp4_00046_2.jpg',\n",
    "#              ]\n",
    "    \n",
    "# Load model\n",
    "model = tf.keras.models.load_model('testing_binary_class_classification/testing_binary_class_classification_model_Adam.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_blinking_lights(img):\n",
    "\n",
    "    if img is None:\n",
    "        print(f\"Error: Unable to load image from {image_path}\")\n",
    "    else:\n",
    "        # Convert the image to grayscale\n",
    "        grey = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        # Apply thresholding to highlight the lights\n",
    "        _, thresholded = cv2.threshold(grey, 200, 255, cv2.THRESH_BINARY)\n",
    "        \n",
    "        # Find contours in the thresholded image\n",
    "        contours, _ = cv2.findContours(thresholded, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        \n",
    "        # Extract ROIs based on contours\n",
    "        rois = []\n",
    "        for contour in contours:\n",
    "            x, y, w, h = cv2.boundingRect(contour)\n",
    "            w = w + 20\n",
    "            h = h + 20\n",
    "            x = x - 10\n",
    "            y = y - 10\n",
    "            \n",
    "            rois.append((x,y,w,h))\n",
    "        \n",
    "        return rois\n",
    "\n",
    "def are_similar_coordinates(coord1, coord2):\n",
    "    x1, y1, w1, h1 = coord1\n",
    "    x2, y2, w2, h2 = coord2\n",
    "    \n",
    "    # Adjust these threshold values based on your requirements\n",
    "    position_threshold = 50  # Adjust as needed\n",
    "    size_threshold = 50  # Adjust as needed\n",
    "    \n",
    "    # Check if the coordinates are similar in terms of position and size\n",
    "    position_diff = abs(x1 - x2) + abs(y1 - y2)\n",
    "    size_diff = abs(w1 - w2) + abs(h1 - h2)\n",
    "    \n",
    "    return position_diff < position_threshold and size_diff < size_threshold\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video Duration: 4.83 seconds\n"
     ]
    }
   ],
   "source": [
    "merged_rois = []\n",
    "all_rois = []\n",
    "\n",
    "cap = cv2.VideoCapture(video)\n",
    "\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "# Calculate the duration of the video in seconds\n",
    "video_duration = total_frames / fps\n",
    "\n",
    "print(f\"Video Duration: {video_duration:.2f} seconds\")\n",
    "\n",
    "framelistVideo = []\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open the video file.\")\n",
    "    exit()\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    img = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    rois = extract_blinking_lights(img)\n",
    "    for roi in rois:\n",
    "        all_rois.append(roi)\n",
    "    \n",
    "    framelistVideo.append(img)\n",
    "\n",
    "if all_rois is not None:\n",
    "    for roi_coordinates in all_rois:\n",
    "        add_to_merged = True\n",
    "\n",
    "        for merged_roi_coordinates in merged_rois:\n",
    "            if are_similar_coordinates(roi_coordinates, merged_roi_coordinates):\n",
    "                add_to_merged = False\n",
    "                break\n",
    "        \n",
    "        if add_to_merged:\n",
    "            merged_rois.append(roi_coordinates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selected_ROIs = []\n",
    "\n",
    "# while True:\n",
    "#     # Select and save ROI manually from the frame\n",
    "#     roi = cv2.selectROI(\"Select the area\", framelistVideo[8], fromCenter=False)\n",
    "\n",
    "#     # Print the selected bounding box coordinates\n",
    "#     print('Selected bounding box: {}'.format(roi))\n",
    "#     selected_ROIs.append(roi)\n",
    "    \n",
    "#     # Delay to allow time for 'q' key to register\n",
    "#     cv2.waitKey(1)\n",
    "    \n",
    "#     # Check for 'q' key press to exit the loop\n",
    "#     if cv2.waitKey(1) == ord('q'):\n",
    "#         cv2.destroyAllWindows()\n",
    "#         break\n",
    "\n",
    "#     cv2.destroyAllWindows()\n",
    "\n",
    "# if len(selected_ROIs) > 0:\n",
    "#     merged_rois = selected_ROIs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_rois = [(493, 283, 52, 53), (763, 256, 55, 41)]\n",
    "\n",
    "# Draw the merged ROIs on the frame\n",
    "for x, y, w, h in merged_rois:\n",
    "    cv2.rectangle(framelistVideo[8], (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "\n",
    "# Display the frame\n",
    "cv2.imshow('frame', framelistVideo[8])\n",
    "\n",
    "# Delay to allow time for 'q' key to register\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 0], [1, 0], [1, 0], [0, 1], [0, 1], [0, 1], [1, 0], [1, 0], [1, 0], [0, 1], [0, 1], [0, 1], [1, 0], [1, 0], [1, 0], [0, 1], [0, 1], [0, 1], [1, 0], [1, 0], [1, 0], [0, 1], [0, 1], [0, 1], [1, 0], [1, 0], [1, 0], [0, 1], [0, 1], [0, 1], [1, 0], [1, 0], [1, 0], [0, 1], [0, 1], [0, 1], [1, 0], [1, 0], [1, 0], [0, 1], [0, 1], [0, 1], [1, 0], [1, 0], [1, 0], [0, 1], [0, 1], [0, 1], [1, 0], [1, 0], [1, 0], [0, 1], [0, 1], [0, 1], [1, 0], [1, 0], [1, 0], [0, 1], [0, 1], [0, 1], [1, 0], [1, 0], [1, 0], [0, 1], [0, 1], [0, 1], [1, 0], [1, 0], [1, 0], [0, 1], [0, 1], [0, 1], [1, 0], [1, 0], [1, 0], [0, 1], [0, 1], [0, 1], [1, 0], [1, 0], [1, 0], [0, 1], [0, 1], [0, 1], [1, 0], [1, 0], [1, 0], [0, 1], [0, 1], [0, 1], [1, 0], [1, 0], [1, 0], [0, 1], [0, 1], [0, 1], [1, 0], [1, 0], [1, 0], [0, 1], [0, 1], [0, 1], [1, 0], [1, 0], [1, 0], [0, 1], [0, 1], [0, 1], [1, 0], [1, 0], [1, 0], [0, 1], [0, 1], [0, 1], [1, 0], [1, 0], [1, 0], [0, 1], [0, 1], [0, 1], [1, 0], [1, 0], [1, 0], [0, 1], [0, 1], [0, 1], [1, 0], [1, 0], [1, 0], [0, 1], [0, 1], [0, 1], [1, 0], [1, 0], [1, 0], [0, 1], [0, 1], [0, 1], [1, 0], [1, 0], [1, 0], [0, 1], [0, 1], [0, 1], [1, 0]]\n"
     ]
    }
   ],
   "source": [
    "rois_in_frames = []\n",
    "for img in framelistVideo:\n",
    "    on_off = []\n",
    "    breaker = False\n",
    "    for roi_coordinates in merged_rois:\n",
    "        x, y, w, h = roi_coordinates\n",
    "        x = x - 30\n",
    "        y = y - 30\n",
    "        w = w + 60\n",
    "        h = h + 60\n",
    "        \n",
    "        cv2.rectangle(img, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "        \n",
    "        # Extract the ROI\n",
    "        roi = img[y:y+h, x:x+w]\n",
    "\n",
    "        # Convert the ROI to grayscale and apply thresholding\n",
    "        grey = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        # Apply thresholding to highlight the lights\n",
    "        _, thresholded = cv2.threshold(grey, 180, 255, cv2.THRESH_BINARY)\n",
    "        \n",
    "        # Find contours in the thresholded image\n",
    "        contours, _ = cv2.findContours(thresholded, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "        if len(contours) == 0:\n",
    "            on_off.append(0)\n",
    "        else:\n",
    "            on_off.append(1)\n",
    "    rois_in_frames.append(on_off)\n",
    "print(rois_in_frames)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1), (0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0)]\n",
      "Frequency of light 144: 5.172413793103448\n",
      "Frequency of light 144: 4.9655172413793105\n"
     ]
    }
   ],
   "source": [
    "# Transpose the array using zip\n",
    "cal_freq = []\n",
    "transposed_array = list(zip(*rois_in_frames))\n",
    "print(transposed_array)\n",
    "for i, separated_array in enumerate(transposed_array):\n",
    "    filtered_tuple = [separated_array[0]] \n",
    "\n",
    "    for i in range(1, len(separated_array)):\n",
    "        if separated_array[i] != separated_array[i - 1]:\n",
    "            filtered_tuple.append(separated_array[i])\n",
    "            \n",
    "    frequency = filtered_tuple.count(1)/video_duration\n",
    "    print(f\"Frequency of light {i}: {frequency}\")\n",
    "    cal_freq.append(frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of blinking lights: 2\n",
      "(493, 283, 52, 53)\n",
      "(763, 256, 55, 41)\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of blinking lights: {len(merged_rois)}')\n",
    "\n",
    "#print all coordinates\n",
    "for i, coordinates in enumerate(merged_rois):\n",
    "    print(coordinates)\n",
    "    #use coordinates to create image roi\n",
    "    x, y, w, h = coordinates\n",
    "    #extract from original image\n",
    "    img = cv2.imread(framelist[0])\n",
    "    roi = img[y:y+h, x:x+w]\n",
    "    #display image\n",
    "    cv2.imshow(f'ROI {i}', roi)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from scipy.fft import fft\n",
    "import concurrent.futures\n",
    "\n",
    "global previous_frame\n",
    "\n",
    "# Function to process each ROI\n",
    "def process_roi(roi):\n",
    "\n",
    "    x, y, w, h = roi\n",
    "    x = x\n",
    "    y = y\n",
    "    w = w\n",
    "    h = h\n",
    "    \n",
    "    roi_processed = frame[y:y+h, x:x+w]\n",
    "    roi_processed = cv2.resize(roi_processed, (30, 30), interpolation=cv2.INTER_LINEAR)\n",
    "    roi_processed = cv2.convertScaleAbs(roi_processed)\n",
    "    roi_processed = np.array(roi_processed)\n",
    "    roi_processed = roi_processed / 255.0\n",
    "    roi_processed = np.expand_dims(roi_processed, axis=0)  # Add batch size dimension\n",
    "\n",
    "    roi_processed_prev = previous_frame[y:y+h, x:x+w]\n",
    "    roi_processed_prev = cv2.resize(roi_processed_prev, (30, 30), interpolation=cv2.INTER_LINEAR)\n",
    "    roi_processed_prev = cv2.convertScaleAbs(roi_processed_prev)\n",
    "    roi_processed_prev = np.array(roi_processed_prev)\n",
    "    roi_processed_prev = roi_processed_prev / 255.0\n",
    "    roi_processed_prev = np.expand_dims(roi_processed_prev, axis=0)  # Add batch size dimension\n",
    "    \n",
    "    return np.concatenate((roi_processed, roi_processed_prev))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions have been started.\n",
      "[(493, 283, 52, 53), (763, 256, 55, 41)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1]\n",
      " [1]\n",
      " [1]\n",
      " [1]]\n",
      "[[1]\n",
      " [1]\n",
      " [1]\n",
      " [1]]\n",
      "[[1]\n",
      " [1]\n",
      " [1]\n",
      " [1]]\n",
      "FPS: 0.43923061804855584\n",
      "Total time: 15.9383953 seconds\n"
     ]
    }
   ],
   "source": [
    "import contextlib\n",
    "import io\n",
    "from datetime import datetime\n",
    "\n",
    "video_path = './vidscapstone/vid12_calibration.mp4'\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"Error opening video file\")\n",
    "    exit()\n",
    "\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)  # Get the frames per second from the video\n",
    "\n",
    "# Initialize roi_frequencies for each ROI\n",
    "roi_frequencies = {i: [] for i in range(len(merged_rois))}\n",
    "reset_interval = 90\n",
    "\n",
    "batch_size = len(merged_rois) * 2\n",
    "frame_count = 0\n",
    "frame_total = 0\n",
    "\n",
    "timer = cv2.getTickCount()\n",
    "\n",
    "print(\"Predictions have been started.\")\n",
    "print(merged_rois)\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    frame_count += 1\n",
    "    frame_total += 1\n",
    "\n",
    "    # Draw the merged ROIs on the frame\n",
    "    for x, y, w, h in merged_rois:\n",
    "        cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "    # show frame\n",
    "    cv2.imshow('frame', frame)\n",
    "    # Wait for a key press (0 means wait indefinitely)\n",
    "    key = cv2.waitKey(0)\n",
    "\n",
    "    # Break the loop if 'q' is pressed or the video ends\n",
    "    if key == ord('q'):\n",
    "        cv2.destroyAllWindows()\n",
    "        break\n",
    "    \n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    if frame_count % 2 != 0 and reset_interval != frame_count:\n",
    "        previous_frame = frame.copy()\n",
    "        continue\n",
    "\n",
    "    # Process ROIs sequentially\n",
    "    rois_processed = [process_roi(roi) for roi in merged_rois]\n",
    "    \n",
    "    rois_processed = np.vstack(rois_processed)  # Stack ROIs to create a batch\n",
    "\n",
    "    # Make predictions on the batch of ROIs\n",
    "    with contextlib.redirect_stdout(io.StringIO()):\n",
    "        predictions = model.predict(rois_processed)\n",
    "        \n",
    "    predictions = np.where(predictions > 0.5, 1, 0)\n",
    "    ordered_predictions_1 = []\n",
    "    ordered_predictions_2 = []\n",
    "    \n",
    "    for i, prediction in enumerate(predictions):\n",
    "        if i%2 == 0:\n",
    "            ordered_predictions_1.append(prediction)\n",
    "        else:\n",
    "            ordered_predictions_2.append(prediction)\n",
    "\n",
    "    predictions = np.concatenate((ordered_predictions_1, ordered_predictions_2))\n",
    "    print(predictions)\n",
    "    # Distribute predictions to each ROI\n",
    "    for i, prediction in enumerate(predictions):\n",
    "        roi_index = i % len(merged_rois)\n",
    "        if len(roi_frequencies[roi_index]) == 0 or roi_frequencies[roi_index][-1] != prediction[0]:\n",
    "            roi_frequencies[roi_index].append(prediction[0])\n",
    "\n",
    "    # Reset frequencies after a certain number of frames\n",
    "    if frame_count % reset_interval == 0:\n",
    "        seconds_passed = frame_count / fps\n",
    "        print(f\"Seconds passed: {seconds_passed:.2f}\")\n",
    "        # Calculate the average frequency for each ROI\n",
    "        for i in range(len(merged_rois)):\n",
    "            print(roi_frequencies[i])\n",
    "            print(seconds_passed)\n",
    "            if len(roi_frequencies[i]) > 0:\n",
    "                frequency = sum(roi_frequencies[i]) / seconds_passed\n",
    "                print(f\"Frequency of light {i}: {frequency:.2f} Hz\")\n",
    "                if frequency < cal_freq[i] - 0.7 or frequency > cal_freq[1] + 0.7:\n",
    "                    print(f\"Inconsistencies on light {i} detected! Please check the light.\")\n",
    "                    print(f\"The calculated Frequency is {frequency:.2f} Hz, while the expected frequency is {cal_freq[i]:.2f} Hz.\")\n",
    "                    with open('logs.txt', 'a') as f:\n",
    "                        f.write(f\"Date & Time: {datetime.now().strftime('%d/%m/%Y, %H:%M:%S')} \\nWarning: Expected frequency of light {i}: {cal_freq[i]:.2f}\\nActual frequency: {frequency:.2f} Hz.\\n\\n\")\n",
    "                        f.close()\n",
    "\n",
    "        roi_frequencies = {i: [] for i in range(len(merged_rois))}\n",
    "        frame_count = 0\n",
    "        print(\"Predicting next batch of frames...\")\n",
    "\n",
    "# Calculate FPS\n",
    "fps = frame_total/((cv2.getTickCount() - timer) / cv2.getTickFrequency())\n",
    "print(f\"FPS: {fps}\")\n",
    "\n",
    "# Calculate total time\n",
    "total_time = (cv2.getTickCount() - timer) / cv2.getTickFrequency()\n",
    "print(f\"Total time: {total_time} seconds\")\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions have been started.\n",
      "[(493, 283, 52, 53), (763, 256, 55, 41)]\n",
      "1/1 [==============================] - 0s 121ms/step\n",
      "[[1]\n",
      " [1]]\n",
      "1/1 [==============================] - 0s 99ms/step\n",
      "[[1]\n",
      " [1]]\n",
      "1/1 [==============================] - 0s 108ms/step\n",
      "[[1]\n",
      " [1]]\n",
      "1/1 [==============================] - 0s 124ms/step\n",
      "[[1]\n",
      " [1]]\n",
      "1/1 [==============================] - 0s 108ms/step\n",
      "[[1]\n",
      " [1]]\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "[[1]\n",
      " [1]]\n",
      "1/1 [==============================] - 0s 118ms/step\n",
      "[[1]\n",
      " [1]]\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "[[1]\n",
      " [1]]\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "[[1]\n",
      " [1]]\n",
      "1/1 [==============================] - 0s 72ms/step\n",
      "[[1]\n",
      " [1]]\n",
      "1/1 [==============================] - 0s 121ms/step\n",
      "[[1]\n",
      " [1]]\n",
      "1/1 [==============================] - 0s 90ms/step\n",
      "[[1]\n",
      " [1]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "need at least one array to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 64\u001b[0m\n\u001b[0;32m     60\u001b[0m     roi_processed \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexpand_dims(roi_processed, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# Add batch size dimension\u001b[39;00m\n\u001b[0;32m     62\u001b[0m     rois_to_batch\u001b[38;5;241m.\u001b[39mappend(roi_processed)\n\u001b[1;32m---> 64\u001b[0m rois_to_batch \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrois_to_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     65\u001b[0m predictions \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(rois_to_batch)\n\u001b[0;32m     66\u001b[0m predictions \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mwhere(predictions \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mvstack\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\numpy\\core\\shape_base.py:296\u001b[0m, in \u001b[0;36mvstack\u001b[1;34m(tup, dtype, casting)\u001b[0m\n\u001b[0;32m    294\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arrs, \u001b[38;5;28mlist\u001b[39m):\n\u001b[0;32m    295\u001b[0m     arrs \u001b[38;5;241m=\u001b[39m [arrs]\n\u001b[1;32m--> 296\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_nx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcasting\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcasting\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mconcatenate\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: need at least one array to concatenate"
     ]
    }
   ],
   "source": [
    "import contextlib\n",
    "import io\n",
    "from datetime import datetime\n",
    "\n",
    "video_path = './vidscapstone/vid12_calibration.mp4'\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"Error opening video file\")\n",
    "    exit()\n",
    "\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)  # Get the frames per second from the video\n",
    "\n",
    "# Initialize roi_frequencies for each ROI\n",
    "roi_frequencies = {i: [] for i in range(len(merged_rois))}\n",
    "reset_interval = 90\n",
    "\n",
    "batch_size = len(merged_rois) * 2\n",
    "frame_count = 0\n",
    "frame_total = 0\n",
    "\n",
    "timer = cv2.getTickCount()\n",
    "\n",
    "print(\"Predictions have been started.\")\n",
    "print(merged_rois)\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    frame_count += 1\n",
    "    frame_total += 1\n",
    "    \n",
    "    # Process ROIs sequentially\n",
    "    rois_to_batch = []\n",
    "    \n",
    "    for r in merged_rois:\n",
    "        x, y, w, h = r\n",
    "\n",
    "        roi_processed = frame[y:y+h, x:x+w]\n",
    "        cv2.imshow('frame', roi_processed)\n",
    "        # Wait for a key press (0 means wait indefinitely)\n",
    "        key = cv2.waitKey(0)\n",
    "\n",
    "        # Break the loop if 'q' is pressed or the video ends\n",
    "        if key == ord('q'):\n",
    "            cv2.destroyAllWindows()\n",
    "            break\n",
    "        \n",
    "        cv2.destroyAllWindows()\n",
    "        roi_processed = cv2.resize(roi_processed, (30, 30), interpolation=cv2.INTER_LINEAR)\n",
    "        roi_processed = cv2.convertScaleAbs(roi_processed)\n",
    "        roi_processed = np.array(roi_processed)\n",
    "        roi_processed = roi_processed / 255.0\n",
    "        roi_processed = np.expand_dims(roi_processed, axis=0)  # Add batch size dimension\n",
    "\n",
    "        rois_to_batch.append(roi_processed)\n",
    "            \n",
    "    rois_to_batch = np.vstack(rois_to_batch)\n",
    "    predictions = model.predict(rois_to_batch)\n",
    "    predictions = np.where(predictions > 0.5, 1, 0)\n",
    "    print(predictions)\n",
    "    # Distribute predictions to each ROI\n",
    "    for i, prediction in enumerate(predictions):\n",
    "        roi_index = i % len(merged_rois)\n",
    "        if len(roi_frequencies[roi_index]) == 0 or roi_frequencies[roi_index][-1] != prediction[0]:\n",
    "            roi_frequencies[roi_index].append(prediction[0])\n",
    "    processed_frames = []\n",
    "    # Reset frequencies after a certain number of frames\n",
    "    if frame_count % reset_interval == 0:\n",
    "        seconds_passed = frame_count / fps\n",
    "        print(f\"Seconds passed: {seconds_passed:.2f}\")\n",
    "        # Calculate the average frequency for each ROI\n",
    "        for i in range(len(merged_rois)):\n",
    "            print(seconds_passed)\n",
    "            if len(roi_frequencies[i]) > 0:\n",
    "                frequency = sum(roi_frequencies[i]) / seconds_passed\n",
    "                print(f\"Frequency of light {i}: {frequency:.2f} Hz\")\n",
    "                if frequency < cal_freq[i] - 0.7 or frequency > cal_freq[1] + 0.7:\n",
    "                    print(f\"Inconsistencies on light {i} detected! Please check the light.\")\n",
    "                    print(f\"The calculated Frequency is {frequency:.2f} Hz, while the expected frequency is {cal_freq[i]:.2f} Hz.\")\n",
    "                    with open('logs.txt', 'a') as f:\n",
    "                        f.write(f\"Date & Time: {datetime.now().strftime('%d/%m/%Y, %H:%M:%S')} \\nWarning: Expected frequency of light {i}: {cal_freq[i]:.2f}\\nActual frequency: {frequency:.2f} Hz.\\n\\n\")\n",
    "                        f.close()\n",
    "\n",
    "        roi_frequencies = {i: [] for i in range(len(merged_rois))}\n",
    "        frame_count = 0\n",
    "        print(\"Predicting next batch of frames...\")\n",
    "\n",
    "# Calculate FPS\n",
    "fps = frame_total/((cv2.getTickCount() - timer) / cv2.getTickFrequency())\n",
    "print(f\"FPS: {fps}\")\n",
    "\n",
    "# Calculate total time\n",
    "total_time = (cv2.getTickCount() - timer) / cv2.getTickFrequency()\n",
    "print(f\"Total time: {total_time} seconds\")\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "beam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
