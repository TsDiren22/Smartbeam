{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "import os\n",
    "from glob import glob\n",
    "import cv2\n",
    "import numpy as np\n",
    "from scipy.fft import fft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load labeled frames and their corresponding labels\n",
    "def load_labeled_frames(data_folder):\n",
    "    images = []\n",
    "    labels = []\n",
    "\n",
    "    for image_path in glob(os.path.join(data_folder, '*.jpg')):\n",
    "        image = cv2.imread(image_path)\n",
    "        image = cv2.resize(image, (30, 30))\n",
    "\n",
    "        label = os.path.splitext(os.path.basename(image_path))[0].split('_')[-1]\n",
    "\n",
    "        images.append(image)\n",
    "\n",
    "        if label == '0':\n",
    "            labels.append(0)\n",
    "        elif label == '1':\n",
    "            labels.append(1)\n",
    "\n",
    "    return images, labels\n",
    "\n",
    "# Preprocess the data\n",
    "def preprocess_data(images, labels):\n",
    "    images = np.array(images)\n",
    "    labels = np.array(labels)\n",
    "    \n",
    "    # duplicate the images and labels\n",
    "    images = np.concatenate((images, images), axis=0)\n",
    "    labels = np.concatenate((labels, labels), axis=0)\n",
    "\n",
    "    # Normalize pixel values to be between 0 and 1\n",
    "    images = images / 255.0\n",
    "\n",
    "    # Shuffle and split the data into training and testing sets\n",
    "    images, labels = shuffle(images, labels, random_state=46)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(images, labels, test_size=0.2, random_state=45)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30, 30, 3)\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "output_folder = 'labeled_frames_binary_test'\n",
    "\n",
    "# Load labeled frames and their corresponding labels\n",
    "images, labels = load_labeled_frames(output_folder)\n",
    "\n",
    "# Preprocess the data\n",
    "X_train, X_test, y_train, y_test = preprocess_data(images, labels)\n",
    "\n",
    "input_shape = X_train[0].shape\n",
    "print(input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1,\n",
       "       1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1,\n",
       "       1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1,\n",
       "       1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "3/3 [==============================] - 3s 323ms/step - loss: 0.6640 - accuracy: 0.5814 - auc: 0.6015 - val_loss: 0.5670 - val_accuracy: 0.6364 - val_auc: 1.0000\n",
      "Epoch 2/30\n",
      "3/3 [==============================] - 0s 62ms/step - loss: 0.5365 - accuracy: 0.6512 - auc: 0.8574 - val_loss: 0.4509 - val_accuracy: 0.6364 - val_auc: 1.0000\n",
      "Epoch 3/30\n",
      "3/3 [==============================] - 0s 58ms/step - loss: 0.5007 - accuracy: 0.6512 - auc: 0.8664 - val_loss: 0.4183 - val_accuracy: 0.6364 - val_auc: 1.0000\n",
      "Epoch 4/30\n",
      "3/3 [==============================] - 0s 57ms/step - loss: 0.4376 - accuracy: 0.6512 - auc: 0.9494 - val_loss: 0.4107 - val_accuracy: 0.6364 - val_auc: 1.0000\n",
      "Epoch 5/30\n",
      "3/3 [==============================] - 0s 49ms/step - loss: 0.3880 - accuracy: 0.6512 - auc: 0.9812 - val_loss: 0.3736 - val_accuracy: 0.6364 - val_auc: 1.0000\n",
      "Epoch 6/30\n",
      "3/3 [==============================] - 0s 50ms/step - loss: 0.3453 - accuracy: 0.6628 - auc: 0.9881 - val_loss: 0.3375 - val_accuracy: 1.0000 - val_auc: 1.0000\n",
      "Epoch 7/30\n",
      "3/3 [==============================] - 0s 48ms/step - loss: 0.3035 - accuracy: 0.7907 - auc: 0.9988 - val_loss: 0.3167 - val_accuracy: 1.0000 - val_auc: 1.0000\n",
      "Epoch 8/30\n",
      "3/3 [==============================] - 0s 52ms/step - loss: 0.2816 - accuracy: 0.7907 - auc: 0.9994 - val_loss: 0.2948 - val_accuracy: 1.0000 - val_auc: 1.0000\n",
      "Epoch 9/30\n",
      "3/3 [==============================] - 0s 71ms/step - loss: 0.2602 - accuracy: 0.8953 - auc: 1.0000 - val_loss: 0.2850 - val_accuracy: 1.0000 - val_auc: 1.0000\n",
      "Epoch 10/30\n",
      "3/3 [==============================] - 0s 62ms/step - loss: 0.2492 - accuracy: 0.9651 - auc: 1.0000 - val_loss: 0.2730 - val_accuracy: 1.0000 - val_auc: 1.0000\n",
      "Epoch 11/30\n",
      "3/3 [==============================] - 0s 60ms/step - loss: 0.2414 - accuracy: 0.9767 - auc: 1.0000 - val_loss: 0.2642 - val_accuracy: 1.0000 - val_auc: 1.0000\n",
      "Epoch 12/30\n",
      "3/3 [==============================] - 0s 59ms/step - loss: 0.2368 - accuracy: 1.0000 - auc: 1.0000 - val_loss: 0.2559 - val_accuracy: 1.0000 - val_auc: 1.0000\n",
      "Epoch 13/30\n",
      "3/3 [==============================] - 0s 57ms/step - loss: 0.2320 - accuracy: 1.0000 - auc: 1.0000 - val_loss: 0.2488 - val_accuracy: 1.0000 - val_auc: 1.0000\n",
      "Epoch 14/30\n",
      "3/3 [==============================] - 0s 62ms/step - loss: 0.2272 - accuracy: 1.0000 - auc: 1.0000 - val_loss: 0.2428 - val_accuracy: 1.0000 - val_auc: 1.0000\n",
      "Epoch 15/30\n",
      "3/3 [==============================] - 0s 58ms/step - loss: 0.2212 - accuracy: 1.0000 - auc: 1.0000 - val_loss: 0.2363 - val_accuracy: 1.0000 - val_auc: 1.0000\n",
      "Epoch 16/30\n",
      "3/3 [==============================] - 0s 63ms/step - loss: 0.2134 - accuracy: 1.0000 - auc: 1.0000 - val_loss: 0.2290 - val_accuracy: 1.0000 - val_auc: 1.0000\n",
      "Epoch 17/30\n",
      "3/3 [==============================] - 0s 66ms/step - loss: 0.2008 - accuracy: 1.0000 - auc: 1.0000 - val_loss: 0.2211 - val_accuracy: 1.0000 - val_auc: 1.0000\n",
      "Epoch 18/30\n",
      "3/3 [==============================] - 0s 64ms/step - loss: 0.1827 - accuracy: 1.0000 - auc: 1.0000 - val_loss: 0.2085 - val_accuracy: 1.0000 - val_auc: 1.0000\n",
      "Epoch 19/30\n",
      "3/3 [==============================] - 0s 74ms/step - loss: 0.1634 - accuracy: 1.0000 - auc: 1.0000 - val_loss: 0.1893 - val_accuracy: 1.0000 - val_auc: 1.0000\n",
      "Epoch 20/30\n",
      "3/3 [==============================] - 0s 64ms/step - loss: 0.1800 - accuracy: 0.9651 - auc: 1.0000 - val_loss: 0.1783 - val_accuracy: 1.0000 - val_auc: 1.0000\n",
      "Epoch 21/30\n",
      "3/3 [==============================] - 0s 71ms/step - loss: 0.2120 - accuracy: 0.9767 - auc: 0.9643 - val_loss: 0.1664 - val_accuracy: 1.0000 - val_auc: 1.0000\n",
      "Epoch 22/30\n",
      "3/3 [==============================] - 0s 84ms/step - loss: 0.1144 - accuracy: 0.9884 - auc: 1.0000 - val_loss: 0.1475 - val_accuracy: 1.0000 - val_auc: 1.0000\n",
      "Epoch 23/30\n",
      "3/3 [==============================] - 0s 64ms/step - loss: 0.1292 - accuracy: 0.9651 - auc: 1.0000 - val_loss: 0.1235 - val_accuracy: 1.0000 - val_auc: 1.0000\n",
      "Epoch 24/30\n",
      "3/3 [==============================] - 0s 76ms/step - loss: 0.0736 - accuracy: 1.0000 - auc: 1.0000 - val_loss: 0.0938 - val_accuracy: 1.0000 - val_auc: 1.0000\n",
      "Epoch 25/30\n",
      "3/3 [==============================] - 0s 69ms/step - loss: 0.0504 - accuracy: 1.0000 - auc: 1.0000 - val_loss: 0.0634 - val_accuracy: 1.0000 - val_auc: 1.0000\n",
      "Epoch 26/30\n",
      "3/3 [==============================] - 0s 65ms/step - loss: 0.0309 - accuracy: 1.0000 - auc: 1.0000 - val_loss: 0.0380 - val_accuracy: 1.0000 - val_auc: 1.0000\n",
      "Epoch 27/30\n",
      "3/3 [==============================] - 0s 65ms/step - loss: 0.0147 - accuracy: 1.0000 - auc: 1.0000 - val_loss: 0.0226 - val_accuracy: 1.0000 - val_auc: 1.0000\n",
      "Epoch 28/30\n",
      "3/3 [==============================] - 0s 82ms/step - loss: 0.0096 - accuracy: 1.0000 - auc: 1.0000 - val_loss: 0.0149 - val_accuracy: 1.0000 - val_auc: 1.0000\n",
      "Epoch 29/30\n",
      "3/3 [==============================] - 0s 75ms/step - loss: 0.0061 - accuracy: 1.0000 - auc: 1.0000 - val_loss: 0.0105 - val_accuracy: 1.0000 - val_auc: 1.0000\n",
      "Epoch 30/30\n",
      "3/3 [==============================] - 0s 64ms/step - loss: 0.0038 - accuracy: 1.0000 - auc: 1.0000 - val_loss: 0.0057 - val_accuracy: 1.0000 - val_auc: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# Build the model\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape))\n",
    "model.add(layers.MaxPooling2D())\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D())\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D())\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(128, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(),\n",
    "              loss='binary_crossentropy',  # Use binary crossentropy for binary classification\n",
    "              metrics=['accuracy', 'AUC'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=30, validation_data=(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 - 3s - loss: 0.0059 - accuracy: 1.0000 - auc: 1.0000 - 3s/epoch - 3s/step\n",
      "\n",
      "Test AUC: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "test_loss, test_acc, test_auc = model.evaluate(X_test, y_test, verbose=2)\n",
    "\n",
    "print('\\nTest AUC:', test_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "framelist = []\n",
    "data_folder = 'labeled_frames_fixed'\n",
    "video = './vidscapstone/vid10.mp4'\n",
    "\n",
    "for image_path in glob(os.path.join(data_folder, '*.jpg')):\n",
    "    framelist.append(image_path)\n",
    "\n",
    "# framelist = [\n",
    "#              'labeled_frames_fixed\\\\vid9.mp4_00054_2.jpg', \n",
    "#              'labeled_frames_fixed\\\\vid9.mp4_00047_0.jpg', \n",
    "#              'labeled_frames_fixed\\\\vid9.mp4_00053_3.jpg', \n",
    "#              'labeled_frames_fixed\\\\vid9.mp4_00046_2.jpg',\n",
    "#              ]\n",
    "    \n",
    "# Load model\n",
    "model = tf.keras.models.load_model('testing_binary_class_classification/testing_binary_class_classification_model_Adam.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_blinking_lights(img):\n",
    "\n",
    "    if img is None:\n",
    "        print(f\"Error: Unable to load image from {image_path}\")\n",
    "    else:\n",
    "        # Convert the image to grayscale\n",
    "        grey = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        # Apply thresholding to highlight the lights\n",
    "        _, thresholded = cv2.threshold(grey, 200, 255, cv2.THRESH_BINARY)\n",
    "        \n",
    "        # Find contours in the thresholded image\n",
    "        contours, _ = cv2.findContours(thresholded, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        \n",
    "        # Extract ROIs based on contours\n",
    "        rois = []\n",
    "        for contour in contours:\n",
    "            x, y, w, h = cv2.boundingRect(contour)\n",
    "            w = w + 20\n",
    "            h = h + 20\n",
    "            x = x - 10\n",
    "            y = y - 10\n",
    "            \n",
    "            rois.append((x,y,w,h))\n",
    "        \n",
    "        return rois\n",
    "\n",
    "def are_similar_coordinates(coord1, coord2):\n",
    "    x1, y1, w1, h1 = coord1\n",
    "    x2, y2, w2, h2 = coord2\n",
    "    \n",
    "    # Adjust these threshold values based on your requirements\n",
    "    position_threshold = 50  # Adjust as needed\n",
    "    size_threshold = 50  # Adjust as needed\n",
    "    \n",
    "    # Check if the coordinates are similar in terms of position and size\n",
    "    position_diff = abs(x1 - x2) + abs(y1 - y2)\n",
    "    size_diff = abs(w1 - w2) + abs(h1 - h2)\n",
    "    \n",
    "    return position_diff < position_threshold and size_diff < size_threshold\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video Duration: 3.13 seconds\n"
     ]
    }
   ],
   "source": [
    "merged_rois = []\n",
    "all_rois = []\n",
    "\n",
    "cap = cv2.VideoCapture(video)\n",
    "\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "# Calculate the duration of the video in seconds\n",
    "video_duration = total_frames / fps\n",
    "\n",
    "print(f\"Video Duration: {video_duration:.2f} seconds\")\n",
    "\n",
    "framelistVideo = []\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open the video file.\")\n",
    "    exit()\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    img = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    rois = extract_blinking_lights(img)\n",
    "    for roi in rois:\n",
    "        all_rois.append(roi)\n",
    "    \n",
    "    framelistVideo.append(img)\n",
    "\n",
    "if all_rois is not None:\n",
    "    for roi_coordinates in all_rois:\n",
    "        add_to_merged = True\n",
    "\n",
    "        for merged_roi_coordinates in merged_rois:\n",
    "            if are_similar_coordinates(roi_coordinates, merged_roi_coordinates):\n",
    "                add_to_merged = False\n",
    "                break\n",
    "        \n",
    "        if add_to_merged:\n",
    "            merged_rois.append(roi_coordinates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 1, 1, 0], [1, 1, 0, 1], [1, 1, 1, 1], [0, 1, 1, 1], [1, 0, 1, 1], [1, 1, 1, 1], [1, 1, 1, 0], [1, 1, 0, 1], [1, 1, 1, 1], [0, 1, 1, 1], [1, 0, 1, 1], [1, 1, 1, 1], [1, 1, 1, 0], [1, 1, 0, 1], [1, 1, 1, 1], [0, 1, 1, 1], [1, 0, 1, 1], [1, 1, 1, 1], [1, 1, 1, 0], [1, 1, 0, 1], [1, 1, 1, 1], [0, 1, 1, 1], [1, 0, 1, 1], [1, 1, 1, 1], [1, 1, 1, 0], [1, 1, 0, 1], [1, 1, 1, 1], [0, 1, 1, 1], [1, 0, 1, 1], [1, 1, 1, 1], [1, 1, 1, 0], [1, 1, 0, 1], [1, 1, 1, 1], [0, 1, 1, 1], [1, 0, 1, 1], [1, 1, 1, 1], [1, 1, 1, 0], [1, 1, 0, 1], [1, 1, 1, 1], [0, 1, 1, 1], [1, 0, 1, 1], [1, 1, 1, 1], [1, 1, 1, 0], [1, 1, 0, 1], [1, 1, 1, 1], [0, 1, 1, 1], [1, 0, 1, 1], [1, 1, 1, 1], [1, 1, 1, 0], [1, 1, 0, 1], [1, 1, 1, 1], [0, 1, 1, 1], [1, 0, 1, 1], [1, 1, 1, 1], [1, 1, 1, 0], [1, 1, 0, 1], [1, 1, 1, 1], [0, 1, 1, 1], [1, 0, 1, 1], [1, 1, 1, 1], [1, 1, 1, 0], [1, 1, 0, 1], [1, 1, 1, 1], [0, 1, 1, 1], [1, 0, 1, 1], [1, 1, 1, 1], [1, 1, 1, 0], [1, 1, 0, 1], [1, 1, 1, 1], [0, 1, 1, 1], [1, 0, 1, 1], [1, 1, 1, 1], [1, 1, 1, 0], [1, 1, 0, 1], [1, 1, 1, 1], [0, 1, 1, 1], [1, 0, 1, 1], [1, 1, 1, 1], [1, 1, 1, 0], [1, 1, 0, 1], [1, 1, 1, 1], [0, 1, 1, 1], [1, 0, 1, 1], [1, 1, 1, 1], [1, 1, 1, 0], [1, 1, 0, 1], [1, 1, 1, 1], [0, 1, 1, 1], [1, 0, 1, 1], [1, 1, 1, 1], [1, 1, 1, 0], [1, 1, 0, 1], [1, 1, 1, 1], [0, 1, 1, 1]]\n"
     ]
    }
   ],
   "source": [
    "rois_in_frames = []\n",
    "for img in framelistVideo:\n",
    "    on_off = []\n",
    "    breaker = False\n",
    "    for roi_coordinates in merged_rois:\n",
    "        x, y, w, h = roi_coordinates\n",
    "        x = x - 30\n",
    "        y = y - 30\n",
    "        w = w + 60\n",
    "        h = h + 60\n",
    "        \n",
    "        cv2.rectangle(img, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "        \n",
    "        # Extract the ROI\n",
    "        roi = img[y:y+h, x:x+w]\n",
    "\n",
    "        # Convert the ROI to grayscale and apply thresholding\n",
    "        grey = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        # Apply thresholding to highlight the lights\n",
    "        _, thresholded = cv2.threshold(grey, 150, 255, cv2.THRESH_BINARY)\n",
    "        \n",
    "        # Find contours in the thresholded image\n",
    "        contours, _ = cv2.findContours(thresholded, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "        if len(contours) == 0:\n",
    "            on_off.append(0)\n",
    "        else:\n",
    "            on_off.append(1)\n",
    "    rois_in_frames.append(on_off)\n",
    "print(rois_in_frames)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequency of light 93: 5.1063829787234045\n",
      "Frequency of light 93: 5.1063829787234045\n",
      "Frequency of light 93: 5.425531914893617\n",
      "Frequency of light 93: 5.1063829787234045\n"
     ]
    }
   ],
   "source": [
    "# Transpose the array using zip\n",
    "transposed_array = list(zip(*rois_in_frames))\n",
    "\n",
    "for i, separated_array in enumerate(transposed_array):\n",
    "    filtered_tuple = [separated_array[0]] \n",
    "\n",
    "    for i in range(1, len(separated_array)):\n",
    "        if separated_array[i] != separated_array[i - 1]:\n",
    "            filtered_tuple.append(separated_array[i])\n",
    "            \n",
    "    frequency = filtered_tuple.count(1)/video_duration\n",
    "    print(f\"Frequency of light {i}: {frequency}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of blinking lights: 4\n",
      "(1374, 773, 27, 27)\n",
      "(458, 283, 27, 27)\n",
      "(1403, 280, 23, 24)\n",
      "(504, 824, 23, 22)\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of blinking lights: {len(merged_rois)}')\n",
    "\n",
    "#print all coordinates\n",
    "for i, coordinates in enumerate(merged_rois):\n",
    "    print(coordinates)\n",
    "    #use coordinates to create image roi\n",
    "    x, y, w, h = coordinates\n",
    "    #extract from original image\n",
    "    img = cv2.imread(framelist[0])\n",
    "    roi = img[y:y+h, x:x+w]\n",
    "    #display image\n",
    "    cv2.imshow(f'ROI {i}', roi)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from scipy.fft import fft\n",
    "import concurrent.futures\n",
    "\n",
    "# Function to process each ROI\n",
    "def process_roi(roi):\n",
    "    x, y, w, h = roi\n",
    "    x = x - 15\n",
    "    y = y - 15\n",
    "    w = w + 30\n",
    "    h = h + 30\n",
    "    \n",
    "    roi_processed = frame[y:y+h, x:x+w]\n",
    "    roi_processed = cv2.resize(roi_processed, (30, 30), interpolation=cv2.INTER_LINEAR)\n",
    "    roi_processed = cv2.convertScaleAbs(roi_processed)\n",
    "    roi_processed = np.array(roi_processed)\n",
    "    roi_processed = roi_processed / 255.0\n",
    "    roi_processed = np.expand_dims(roi_processed, axis=0)\n",
    "\n",
    "    \n",
    "    return roi_processed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 38ms/step\n",
      "1\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "2\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "3\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "4\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "5\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "6\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "7\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "8\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "9\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "10\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "11\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "12\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "13\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "14\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "15\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "16\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "17\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "18\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "19\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "20\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "21\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "22\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "23\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "24\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "25\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "26\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "27\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "28\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "29\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "30\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "31\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "32\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "33\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "34\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "35\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "36\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "37\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "38\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "39\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "40\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "41\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "42\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "43\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "44\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "45\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "46\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "47\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "48\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "49\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "50\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "51\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "52\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "53\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "54\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "55\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "56\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "57\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "58\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "59\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "60\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "61\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "62\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "63\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "64\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "65\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "66\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "67\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "68\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "69\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "70\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "71\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "72\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "73\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "74\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "75\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "76\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "77\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "78\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "79\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "80\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "81\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "82\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "83\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "84\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "85\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "86\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "87\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "88\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "89\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "90\n",
      "Seconds passed: 3.00\n",
      "[1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1]\n",
      "Frequency of light 0: 5.33 Hz\n",
      "[1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1]\n",
      "Frequency of light 1: 5.33 Hz\n",
      "[1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1]\n",
      "Frequency of light 2: 5.33 Hz\n",
      "[0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1]\n",
      "Frequency of light 3: 5.00 Hz\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "2\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "3\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "video_path = './vidscapstone/vid10.mp4'\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"Error opening video file\")\n",
    "    exit()\n",
    "\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)  # Get the frames per second from the video\n",
    "\n",
    "# Initialize roi_frequencies for each ROI\n",
    "roi_frequencies = {i: [] for i in range(len(merged_rois))}\n",
    "reset_interval = 90\n",
    "\n",
    "frame_count = 0\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Process ROIs in parallel\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        rois_processed = list(executor.map(process_roi, merged_rois))\n",
    "\n",
    "    rois_processed = np.vstack(rois_processed)  # Stack ROIs to create a batch\n",
    "\n",
    "    # Make predictions on the batch of ROIs\n",
    "    predictions = model.predict(rois_processed)\n",
    "    predictions = np.where(predictions > 0.5, 1, 0)\n",
    "\n",
    "    # Distribute predictions to each ROI\n",
    "    for i, prediction in enumerate(predictions):\n",
    "        if len(roi_frequencies[i]) == 0 or roi_frequencies[i][-1] != prediction[0]:\n",
    "            roi_frequencies[i].append(prediction[0])\n",
    "\n",
    "\n",
    "    frame_count += 1\n",
    "    print(frame_count)\n",
    "\n",
    "    # Reset frequencies after a certain number of frames\n",
    "    if frame_count % reset_interval == 0:\n",
    "        seconds_passed = frame_count / fps\n",
    "        print(f\"Seconds passed: {seconds_passed:.2f}\")\n",
    "        # Calculate the average frequency for each ROI\n",
    "        for i in range(len(merged_rois)):\n",
    "            print(roi_frequencies[i])\n",
    "            if len(roi_frequencies[i]) > 0:\n",
    "                frequency = sum(roi_frequencies[i]) / seconds_passed\n",
    "                print(f\"Frequency of light {i}: {frequency:.2f} Hz\")\n",
    "\n",
    "        roi_frequencies = {i: [] for i in range(len(merged_rois))}\n",
    "        frame_count = 0\n",
    "\n",
    "cap.release()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\diren\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\engine\\training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "directory = 'testing_binary_class_classification/'\n",
    "\n",
    "model.save(directory + 'testing_binary_class_classification_model_Adam.h5')\n",
    "\n",
    "with open(directory + 'testing_binary_class_classification_model_Adam_history', 'wb') as file_pi:\n",
    "    pickle.dump(history, file_pi)\n",
    "\n",
    "with open(directory + 'testing_binary_class_classification_model_Adam_X_test', 'wb') as file_pi:\n",
    "    pickle.dump(X_test, file_pi)\n",
    "\n",
    "with open(directory + 'testing_binary_class_classification_model_Adam_y_test', 'wb') as file_pi:\n",
    "    pickle.dump(y_test, file_pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "beam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
