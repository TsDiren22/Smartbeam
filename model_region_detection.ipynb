{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "import os\n",
    "from glob import glob\n",
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load labeled frames and their corresponding labels\n",
    "def load_labeled_frames(data_folder):\n",
    "    images = []\n",
    "    labels = []\n",
    "\n",
    "    for image_path in glob(os.path.join(data_folder, '*.jpg')):\n",
    "        image = cv2.imread(image_path)\n",
    "        image = cv2.resize(image, (20, 20))\n",
    "\n",
    "        label = os.path.splitext(os.path.basename(image_path))[0].split('_')[-1]\n",
    "\n",
    "        images.append(image)\n",
    "\n",
    "        if label == '0':\n",
    "            labels.append(0)\n",
    "        elif label == '1':\n",
    "            labels.append(1)\n",
    "\n",
    "    return images, labels\n",
    "\n",
    "# Preprocess the data\n",
    "def preprocess_data(images, labels):\n",
    "    images = np.array(images)\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    # Normalize pixel values to be between 0 and 1\n",
    "    images = images / 255.0\n",
    "\n",
    "    # Shuffle and split the data into training and testing sets\n",
    "    images, labels = shuffle(images, labels, random_state=46)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(images, labels, test_size=0.2, random_state=45)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "output_folder = 'labeled_frames_binary_test'\n",
    "\n",
    "# Load labeled frames and their corresponding labels\n",
    "images, labels = load_labeled_frames(output_folder)\n",
    "\n",
    "# Preprocess the data\n",
    "X_train, X_test, y_train, y_test = preprocess_data(images, labels)\n",
    "\n",
    "input_shape = X_train[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "1/1 [==============================] - 8s 8s/step - loss: 0.7095 - accuracy: 0.2105 - auc: 0.0667 - val_loss: 0.6420 - val_accuracy: 0.8000 - val_auc: 1.0000\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 239ms/step - loss: 0.6652 - accuracy: 0.6316 - auc: 0.8444 - val_loss: 0.5909 - val_accuracy: 0.8000 - val_auc: 1.0000\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 171ms/step - loss: 0.6292 - accuracy: 0.6316 - auc: 0.9111 - val_loss: 0.5487 - val_accuracy: 0.8000 - val_auc: 1.0000\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 204ms/step - loss: 0.6016 - accuracy: 0.6316 - auc: 0.9111 - val_loss: 0.5155 - val_accuracy: 0.8000 - val_auc: 1.0000\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 178ms/step - loss: 0.5845 - accuracy: 0.6316 - auc: 0.9111 - val_loss: 0.4912 - val_accuracy: 0.8000 - val_auc: 1.0000\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 170ms/step - loss: 0.5749 - accuracy: 0.6316 - auc: 0.9111 - val_loss: 0.4729 - val_accuracy: 0.8000 - val_auc: 1.0000\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 251ms/step - loss: 0.5676 - accuracy: 0.6316 - auc: 0.9111 - val_loss: 0.4583 - val_accuracy: 0.8000 - val_auc: 1.0000\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 235ms/step - loss: 0.5584 - accuracy: 0.6316 - auc: 0.9333 - val_loss: 0.4466 - val_accuracy: 0.8000 - val_auc: 1.0000\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 228ms/step - loss: 0.5464 - accuracy: 0.6316 - auc: 0.9611 - val_loss: 0.4375 - val_accuracy: 0.8000 - val_auc: 1.0000\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 195ms/step - loss: 0.5329 - accuracy: 0.6316 - auc: 1.0000 - val_loss: 0.4301 - val_accuracy: 0.8000 - val_auc: 1.0000\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 317ms/step - loss: 0.5203 - accuracy: 0.6316 - auc: 1.0000 - val_loss: 0.4244 - val_accuracy: 0.8000 - val_auc: 1.0000\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 268ms/step - loss: 0.5085 - accuracy: 0.6316 - auc: 1.0000 - val_loss: 0.4187 - val_accuracy: 0.8000 - val_auc: 1.0000\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 235ms/step - loss: 0.4984 - accuracy: 0.6316 - auc: 1.0000 - val_loss: 0.4115 - val_accuracy: 0.8000 - val_auc: 1.0000\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 215ms/step - loss: 0.4879 - accuracy: 0.6316 - auc: 1.0000 - val_loss: 0.4016 - val_accuracy: 0.8000 - val_auc: 1.0000\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 216ms/step - loss: 0.4764 - accuracy: 0.7895 - auc: 1.0000 - val_loss: 0.3880 - val_accuracy: 1.0000 - val_auc: 1.0000\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 249ms/step - loss: 0.4633 - accuracy: 0.7895 - auc: 1.0000 - val_loss: 0.3712 - val_accuracy: 1.0000 - val_auc: 1.0000\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 302ms/step - loss: 0.4497 - accuracy: 0.7895 - auc: 1.0000 - val_loss: 0.3536 - val_accuracy: 1.0000 - val_auc: 1.0000\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 341ms/step - loss: 0.4352 - accuracy: 0.7895 - auc: 1.0000 - val_loss: 0.3376 - val_accuracy: 1.0000 - val_auc: 1.0000\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 309ms/step - loss: 0.4200 - accuracy: 0.7895 - auc: 1.0000 - val_loss: 0.3226 - val_accuracy: 1.0000 - val_auc: 1.0000\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 239ms/step - loss: 0.4042 - accuracy: 0.7895 - auc: 1.0000 - val_loss: 0.3079 - val_accuracy: 1.0000 - val_auc: 1.0000\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 274ms/step - loss: 0.3875 - accuracy: 0.7895 - auc: 1.0000 - val_loss: 0.2935 - val_accuracy: 1.0000 - val_auc: 1.0000\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 272ms/step - loss: 0.3689 - accuracy: 1.0000 - auc: 1.0000 - val_loss: 0.2776 - val_accuracy: 1.0000 - val_auc: 1.0000\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 266ms/step - loss: 0.3478 - accuracy: 1.0000 - auc: 1.0000 - val_loss: 0.2602 - val_accuracy: 1.0000 - val_auc: 1.0000\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 283ms/step - loss: 0.3266 - accuracy: 1.0000 - auc: 1.0000 - val_loss: 0.2366 - val_accuracy: 1.0000 - val_auc: 1.0000\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 335ms/step - loss: 0.3028 - accuracy: 1.0000 - auc: 1.0000 - val_loss: 0.2130 - val_accuracy: 1.0000 - val_auc: 1.0000\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 227ms/step - loss: 0.2800 - accuracy: 1.0000 - auc: 1.0000 - val_loss: 0.1905 - val_accuracy: 1.0000 - val_auc: 1.0000\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 202ms/step - loss: 0.2566 - accuracy: 1.0000 - auc: 1.0000 - val_loss: 0.1721 - val_accuracy: 1.0000 - val_auc: 1.0000\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 342ms/step - loss: 0.2326 - accuracy: 1.0000 - auc: 1.0000 - val_loss: 0.1570 - val_accuracy: 1.0000 - val_auc: 1.0000\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 320ms/step - loss: 0.2087 - accuracy: 1.0000 - auc: 1.0000 - val_loss: 0.1399 - val_accuracy: 1.0000 - val_auc: 1.0000\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 306ms/step - loss: 0.1849 - accuracy: 1.0000 - auc: 1.0000 - val_loss: 0.1190 - val_accuracy: 1.0000 - val_auc: 1.0000\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 346ms/step - loss: 0.1604 - accuracy: 1.0000 - auc: 1.0000 - val_loss: 0.1015 - val_accuracy: 1.0000 - val_auc: 1.0000\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 1s 522ms/step - loss: 0.1374 - accuracy: 1.0000 - auc: 1.0000 - val_loss: 0.0875 - val_accuracy: 1.0000 - val_auc: 1.0000\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 257ms/step - loss: 0.1152 - accuracy: 1.0000 - auc: 1.0000 - val_loss: 0.0742 - val_accuracy: 1.0000 - val_auc: 1.0000\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 272ms/step - loss: 0.0949 - accuracy: 1.0000 - auc: 1.0000 - val_loss: 0.0603 - val_accuracy: 1.0000 - val_auc: 1.0000\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 404ms/step - loss: 0.0762 - accuracy: 1.0000 - auc: 1.0000 - val_loss: 0.0491 - val_accuracy: 1.0000 - val_auc: 1.0000\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - 0s 268ms/step - loss: 0.0607 - accuracy: 1.0000 - auc: 1.0000 - val_loss: 0.0404 - val_accuracy: 1.0000 - val_auc: 1.0000\n",
      "Epoch 37/40\n",
      "1/1 [==============================] - 0s 274ms/step - loss: 0.0480 - accuracy: 1.0000 - auc: 1.0000 - val_loss: 0.0330 - val_accuracy: 1.0000 - val_auc: 1.0000\n",
      "Epoch 38/40\n",
      "1/1 [==============================] - 0s 334ms/step - loss: 0.0379 - accuracy: 1.0000 - auc: 1.0000 - val_loss: 0.0265 - val_accuracy: 1.0000 - val_auc: 1.0000\n",
      "Epoch 39/40\n",
      "1/1 [==============================] - 0s 364ms/step - loss: 0.0296 - accuracy: 1.0000 - auc: 1.0000 - val_loss: 0.0213 - val_accuracy: 1.0000 - val_auc: 1.0000\n",
      "Epoch 40/40\n",
      "1/1 [==============================] - 1s 521ms/step - loss: 0.0234 - accuracy: 1.0000 - auc: 1.0000 - val_loss: 0.0172 - val_accuracy: 1.0000 - val_auc: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# Build the model\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu', input_shape=input_shape))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu'))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(1, activation='sigmoid')) \n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(),\n",
    "              loss='binary_crossentropy',  # Use binary crossentropy for binary classification\n",
    "              metrics=['accuracy', 'AUC'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=40, validation_data=(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 - 0s - loss: 0.0172 - accuracy: 1.0000 - auc: 1.0000 - 57ms/epoch - 57ms/step\n",
      "\n",
      "Test AUC: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "test_loss, test_acc, test_auc = model.evaluate(X_test, y_test, verbose=2)\n",
    "\n",
    "print('\\nTest AUC:', test_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 38ms/step\n",
      "[[0.9999981]]\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "[[0.9999987]]\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "[[0.9997089]]\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "[[0.06282663]]\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "[[0.9971877]]\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "[[0.99999917]]\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "[[0.9997112]]\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "[[0.99967235]]\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Open the video file\n",
    "video_path = './vidscapstone/vid9.mp4'\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Check if the video file is opened successfully\n",
    "if not cap.isOpened():\n",
    "    print(\"Error opening video file\")\n",
    "    exit()\n",
    "\n",
    "rois = [(1374, 773, 27, 27), (458, 283, 27, 27), (1403, 280, 23, 24), (504, 824, 23, 22)]\n",
    "\n",
    "# Loop through frames\n",
    "while True:\n",
    "    # Read a frame\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # Break the loop if the video is finished\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    count = 0\n",
    "\n",
    "    for roi in rois:\n",
    "        x, y, w, h = roi\n",
    "        roi_processed = frame[y:y+h, x:x+w]\n",
    "        roi_processed = cv2.resize(roi_processed, (20, 20))\n",
    "        roi_processed = cv2.convertScaleAbs(roi_processed)\n",
    "        roi_processed = np.array(roi_processed)\n",
    "        roi_processed = roi_processed / 255.0\n",
    "        roi_processed = np.expand_dims(roi_processed, axis=0)\n",
    "        pred = model.predict(roi_processed)\n",
    "        print(pred)\n",
    "        if pred > 0.5:\n",
    "            count += 1\n",
    "\n",
    "    #frame_to_show = cv2.resize(frame, (0, 0), fx=0.5, fy=0.5)\n",
    "    #cv2.imshow(f'Turned on lights {count}', frame_to_show)\n",
    "    \n",
    "    #show each roi\n",
    "    for roi in rois:\n",
    "        x, y, w, h = roi\n",
    "        cv2.imshow(f'ROI {x}, {y}', frame[y:y+h, x:x+w])\n",
    "\n",
    "        breaker = False\n",
    "        # Press q to exit\n",
    "        if cv2.waitKey(0) & 0xFF == ord('q'):\n",
    "            cv2.destroyAllWindows()\n",
    "            breaker = True\n",
    "            break\n",
    "        else:\n",
    "            cv2.destroyAllWindows()\n",
    "\n",
    "    if breaker:\n",
    "        break\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "# Release the video capture object\n",
    "cap.release()\n",
    "\n",
    "# Close all OpenCV windows\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "directory = 'testing_binary_class_classification/'\n",
    "\n",
    "model.save(directory + 'testing_binary_class_classification_model_Adam.h5')\n",
    "\n",
    "with open(directory + 'testing_binary_class_classification_model_Adam_history', 'wb') as file_pi:\n",
    "    pickle.dump(history, file_pi)\n",
    "\n",
    "with open(directory + 'testing_binary_class_classification_model_Adam_X_test', 'wb') as file_pi:\n",
    "    pickle.dump(X_test, file_pi)\n",
    "\n",
    "with open(directory + 'testing_binary_class_classification_model_Adam_y_test', 'wb') as file_pi:\n",
    "    pickle.dump(y_test, file_pi)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "beam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
